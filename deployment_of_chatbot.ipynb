{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deployment-of-chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj_bPuri3G5R"
      },
      "source": [
        "# 自然语言处理 （NLP-Notebook）\n",
        "  作者：朱凡\n",
        "\n",
        "  时间：2021/11/23\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUCAPlUlju1E"
      },
      "source": [
        "## 数据准备"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggd8p_qZkLGK",
        "outputId": "8591427c-9e57-4ed3-e6d9-59ef11076214"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s git://github.com/tensorlayer/seq2seq-chatbot.git seq2seq-chatbot\n",
        "%cd cloned-repo\n",
        "!ls\n",
        "%cd seq2seq-chatbot/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'seq2seq-chatbot' already exists and is not an empty directory.\n",
            "[Errno 2] No such file or directory: 'cloned-repo'\n",
            "/content\n",
            "sample_data  seq2seq-chatbot\n",
            "/content/seq2seq-chatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WqRfMWdkncr"
      },
      "source": [
        "## 安装相关库"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfWg8vrYkrDz",
        "outputId": "4108d776-a859-469f-97e9-681c0e9f3222"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: tensorlayer in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.19.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.62.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.37.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.42.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.22.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (12.0.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->-r requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: cloudpickle>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from tensorlayer->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorlayer->-r requirements.txt (line 3)) (0.18.3)\n",
            "Requirement already satisfied: imageio>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorlayer->-r requirements.txt (line 3)) (2.12.0)\n",
            "Requirement already satisfied: progressbar2>=3.39.3 in /usr/local/lib/python3.7/dist-packages (from tensorlayer->-r requirements.txt (line 3)) (3.55.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio>=2.5.0->tensorlayer->-r requirements.txt (line 3)) (8.4.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2>=3.39.3->tensorlayer->-r requirements.txt (line 3)) (2.5.6)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (2021.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (1.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkUOgOEuj4ai"
      },
      "source": [
        "## 导入相关库"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o77XgHRkRcP"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "import numpy as np\n",
        "from tensorlayer.cost import cross_entropy_seq, cross_entropy_seq_with_mask\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from data.twitter import data\n",
        "from tensorlayer.models.seq2seq import Seq2seq\n",
        "from tensorlayer.models.seq2seq_with_attention import Seq2seqLuongAttention\n",
        "import os"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFkv5cU2jyik"
      },
      "source": [
        "## 定义函数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ2VmwNRlNkG"
      },
      "source": [
        "### 定义初始化函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJDRCttgi7Pk"
      },
      "source": [
        "\n",
        "def initial_setup(data_corpus):\n",
        "    metadata, idx_q, idx_a = data.load_data(PATH='data/{}/'.format(data_corpus))\n",
        "    (trainX, trainY), (testX, testY), (validX, validY) = data.split_dataset(idx_q, idx_a)\n",
        "    trainX = tl.prepro.remove_pad_sequences(trainX.tolist())\n",
        "    trainY = tl.prepro.remove_pad_sequences(trainY.tolist())\n",
        "    testX = tl.prepro.remove_pad_sequences(testX.tolist())\n",
        "    testY = tl.prepro.remove_pad_sequences(testY.tolist())\n",
        "    validX = tl.prepro.remove_pad_sequences(validX.tolist())\n",
        "    validY = tl.prepro.remove_pad_sequences(validY.tolist())\n",
        "    return metadata, trainX, trainY, testX, testY, validX, validY\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgbt1CFWnpzf"
      },
      "source": [
        "### 入口"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "W8Zy3sbenpQ6",
        "outputId": "4f02a063-c3b5-4a6b-aa8e-a20a2125163c"
      },
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data_corpus = \"twitter\"\n",
        "\n",
        "    #data preprocessing\n",
        "    metadata, trainX, trainY, testX, testY, validX, validY = initial_setup(data_corpus)\n",
        "\n",
        "    # Parameters\n",
        "    src_len = len(trainX)\n",
        "    tgt_len = len(trainY)\n",
        "\n",
        "    assert src_len == tgt_len\n",
        "\n",
        "    batch_size = 32\n",
        "    n_step = src_len // batch_size\n",
        "    src_vocab_size = len(metadata['idx2w']) # 8002 (0~8001)\n",
        "    emb_dim = 1024\n",
        "\n",
        "    word2idx = metadata['w2idx']   # dict  word 2 index\n",
        "    idx2word = metadata['idx2w']   # list index 2 word\n",
        "\n",
        "    unk_id = word2idx['unk']   # 1\n",
        "    pad_id = word2idx['_']     # 0\n",
        "\n",
        "    start_id = src_vocab_size  # 8002\n",
        "    end_id = src_vocab_size + 1  # 8003\n",
        "\n",
        "    word2idx.update({'start_id': start_id})\n",
        "    word2idx.update({'end_id': end_id})\n",
        "    idx2word = idx2word + ['start_id', 'end_id']\n",
        "\n",
        "    src_vocab_size = tgt_vocab_size = src_vocab_size + 2\n",
        "\n",
        "    num_epochs = 1    # 迭代轮数 初始为50；\n",
        "    vocabulary_size = src_vocab_size\n",
        "    \n",
        "\n",
        "\n",
        "    def inference(seed, top_n):\n",
        "        model_.eval()\n",
        "        seed_id = [word2idx.get(w, unk_id) for w in seed.split(\" \")]\n",
        "        sentence_id = model_(inputs=[[seed_id]], seq_length=20, start_token=start_id, top_n = top_n)\n",
        "        sentence = []\n",
        "        for w_id in sentence_id[0]:\n",
        "            w = idx2word[w_id]\n",
        "            if w == 'end_id':\n",
        "                break\n",
        "            sentence = sentence + [w]\n",
        "        return sentence\n",
        "\n",
        "    decoder_seq_length = 20\n",
        "    model_ = Seq2seq(\n",
        "        decoder_seq_length = decoder_seq_length,\n",
        "        cell_enc=tf.keras.layers.GRUCell,\n",
        "        cell_dec=tf.keras.layers.GRUCell,\n",
        "        n_layer=3,\n",
        "        n_units=256,\n",
        "        embedding_layer=tl.layers.Embedding(vocabulary_size=vocabulary_size, embedding_size=emb_dim),\n",
        "        )\n",
        "    \n",
        "\n",
        "    # Uncomment below statements if you have already saved the model\n",
        "\n",
        "    # load_weights = tl.files.load_npz(name='model.npz')\n",
        "    # tl.files.assign_weights(load_weights, model_)\n",
        "\n",
        "    optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "    model_.train()\n",
        "\n",
        "    seeds = [\"happy birthday have a nice day\",\n",
        "                 \"donald trump won last nights presidential debate according to snap online polls\"]\n",
        "    for epoch in range(num_epochs):\n",
        "        model_.train()\n",
        "        trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
        "        total_loss, n_iter = 0, 0\n",
        "        for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), \n",
        "                        total=n_step, desc='Epoch[{}/{}]'.format(epoch + 1, num_epochs), leave=False):\n",
        "\n",
        "            X = tl.prepro.pad_sequences(X)\n",
        "            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n",
        "            _target_seqs = tl.prepro.pad_sequences(_target_seqs, maxlen=decoder_seq_length)\n",
        "            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n",
        "            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs, maxlen=decoder_seq_length)\n",
        "            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                ## compute outputs\n",
        "                output = model_(inputs = [X, _decode_seqs])\n",
        "                \n",
        "                output = tf.reshape(output, [-1, vocabulary_size])\n",
        "                ## compute loss and update model\n",
        "                loss = cross_entropy_seq_with_mask(logits=output, target_seqs=_target_seqs, input_mask=_target_mask)\n",
        "\n",
        "                grad = tape.gradient(loss, model_.all_weights)\n",
        "                optimizer.apply_gradients(zip(grad, model_.all_weights))\n",
        "            \n",
        "            total_loss += loss\n",
        "            n_iter += 1\n",
        "\n",
        "        # printing average loss after every epoch\n",
        "        print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, num_epochs, total_loss / n_iter))\n",
        "\n",
        "        for seed in seeds:\n",
        "            print(\"Query >\", seed)\n",
        "            top_n = 3\n",
        "            for i in range(top_n):\n",
        "                sentence = inference(seed, top_n)\n",
        "                print(\" >\", ' '.join(sentence))\n",
        "\n",
        "        tl.files.save_npz(model_.all_weights, name='model.npz')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7dd7633c0ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#data preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'initial_setup' is not defined"
          ]
        }
      ]
    }
  ]
}