{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj_bPuri3G5R"
      },
      "source": [
        "# 自然语言处理 （NLP-Notebook）\n",
        "  作者：[Alafun](https://github.com/Alafun)\n",
        "\n",
        "  时间：2021/11/23\n",
        "\n",
        "  描述：基于 `Seq2seq` （编码器-解码器模型） 的聊天机器人\n",
        "  \n",
        "  <details>\n",
        "    <summary><strong>Seq2seq</strong></summary>\n",
        "      <p>\n",
        "        Seq2Seq（是 Sequence-to-sequence 的缩写），就如字面意思，输入一个序列，输出另一个序列。这种结构最重要的地方在于输入序列和输出序列的长度是可变的。🤗\n",
        "      </p>\n",
        "  </details>\n",
        "\n",
        "  <img src=\"https://pytorch.org/tutorials/_images/seq2seq_ts.png\" loading=\"lazy\" alt=\"Overview\" width=\"500\" />\n",
        "\n",
        "  \n",
        "  >[一文看懂 Encoder-Decoder 和 Seq2Seq ](https://easyai.tech/ai-definition/encoder-decoder-seq2seq/)\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUCAPlUlju1E"
      },
      "source": [
        "## 数据准备"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_IzIKbPsoyD"
      },
      "source": [
        "#### 如果你有 GoogleDrive 的账号可以先运行这段代码\n",
        "\n",
        "目的为了将数据保持到 GoogleDrive 方便存储"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpATrjq9sm_d",
        "outputId": "e8b62ef3-da58-483a-bc32-09c9ad386116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "# then click the link and enter you code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4B11Hk2ty_-",
        "outputId": "6afda3d5-f5c1-4774-cb93-7161368676e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/seq2seq-chatbot\n"
          ]
        }
      ],
      "source": [
        "%cd /gdrive/MyDrive/\n",
        "# 可能不完全一样"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w9O4gxYs0V4"
      },
      "source": [
        "#### 如果你没有谷歌账号，不要管上面的程序块，直接跳到这个cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggd8p_qZkLGK",
        "outputId": "0c512527-7aa8-45cb-e08e-3306ceeca58c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'seq2seq-chatbot'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 238, done.\u001b[K\n",
            "remote: Total 238 (delta 0), reused 0 (delta 0), pack-reused 238\u001b[K\n",
            "Receiving objects: 100% (238/238), 15.00 MiB | 7.52 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n",
            "/content/drive/MyDrive/seq2seq-chatbot\n",
            "data  main.py  README.md  requirements.txt\n",
            "[Errno 2] No such file or directory: 'seq2seq-chatbot/'\n",
            "/content/drive/MyDrive/seq2seq-chatbot\n"
          ]
        }
      ],
      "source": [
        "# Clone the entire repo.\n",
        "# 每次运行session 只需执行一次\n",
        "!git clone -l -s git://github.com/tensorlayer/seq2seq-chatbot.git seq2seq-chatbot\n",
        "%cd seq2seq-chatbot\n",
        "!ls\n",
        "%cd seq2seq-chatbot/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WqRfMWdkncr"
      },
      "source": [
        "## 安装相关库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfWg8vrYkrDz",
        "outputId": "171beabe-f336-4c5a-b48f-b201a9206529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: tensorlayer in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.19.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.62.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.42.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.22.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (12.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.13.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.37.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.7.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->-r requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: progressbar2>=3.39.3 in /usr/local/lib/python3.7/dist-packages (from tensorlayer->-r requirements.txt (line 3)) (3.55.0)\n",
            "Requirement already satisfied: cloudpickle>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from tensorlayer->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorlayer->-r requirements.txt (line 3)) (0.18.3)\n",
            "Requirement already satisfied: imageio>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorlayer->-r requirements.txt (line 3)) (2.12.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio>=2.5.0->tensorlayer->-r requirements.txt (line 3)) (8.4.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2>=3.39.3->tensorlayer->-r requirements.txt (line 3)) (2.5.6)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (2021.11.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (1.3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkUOgOEuj4ai"
      },
      "source": [
        "## 导入相关库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o77XgHRkRcP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "import numpy as np\n",
        "from tensorlayer.cost import cross_entropy_seq, cross_entropy_seq_with_mask #\n",
        "from tqdm import tqdm # 进度条 https://pypi.org/project/tqdm/ tqdm 源自阿拉伯语单词 taqaddum (تقدّم)，意思是“进步”，是西班牙语 (te quiero demasiado) 中“我非常爱你”的缩写。\n",
        "from sklearn.utils import shuffle # shuffle\n",
        "from data.twitter import data # 数据\n",
        "from tensorlayer.models.seq2seq import Seq2seq # seq2seq\n",
        "from tensorlayer.models.seq2seq_with_attention import Seq2seqLuongAttention # 引入attention\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFkv5cU2jyik"
      },
      "source": [
        "## 定义函数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ2VmwNRlNkG"
      },
      "source": [
        "### 定义初始化函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJDRCttgi7Pk"
      },
      "outputs": [],
      "source": [
        "def initial_setup(data_corpus):\n",
        "  \"\"\"\n",
        "  加载数据\n",
        "  将语料库的数据分配给参数\n",
        "  \"\"\"\n",
        "    metadata, idx_q, idx_a = data.load_data(PATH='data/{}/'.format(data_corpus))\n",
        "    (trainX, trainY), (testX, testY), (validX, validY) = data.split_dataset(idx_q, idx_a)\n",
        "    trainX = tl.prepro.remove_pad_sequences(trainX.tolist())\n",
        "    trainY = tl.prepro.remove_pad_sequences(trainY.tolist())\n",
        "    testX = tl.prepro.remove_pad_sequences(testX.tolist())\n",
        "    testY = tl.prepro.remove_pad_sequences(testY.tolist())\n",
        "    validX = tl.prepro.remove_pad_sequences(validX.tolist())\n",
        "    validY = tl.prepro.remove_pad_sequences(validY.tolist())\n",
        "    return metadata, trainX, trainY, testX, testY, validX, validY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgbt1CFWnpzf"
      },
      "source": [
        "### 入口"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8Zy3sbenpQ6",
        "outputId": "6680f8cb-2055-49ca-a22e-f5f7a7fbb55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TL] Embedding embedding_1: (8004, 1024)\n",
            "[TL] RNN rnn_1: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_2: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_3: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_4: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_5: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_6: cell: GRUCell, n_units: 256\n",
            "[TL] Reshape reshape_1\n",
            "[TL] Dense  dense_1: 8004 No Activation\n",
            "[TL] Reshape reshape_2\n",
            "[TL] Reshape reshape_3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch[1/1]:  97%|█████████▋| 2755/2852 [25:34<00:53,  1.81it/s]"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    data_corpus = \"twitter\"  # 定义corpus为twitter可换成自己的语料库\n",
        "\n",
        "    #data preprocessing\n",
        "    metadata, trainX, trainY, testX, testY, validX, validY = initial_setup(data_corpus)\n",
        "\n",
        "    # Parameters\n",
        "    src_len = len(trainX)\n",
        "    tgt_len = len(trainY)\n",
        "\n",
        "    assert src_len == tgt_len\n",
        "\n",
        "    batch_size = 32\n",
        "    n_step = src_len // batch_size\n",
        "    src_vocab_size = len(metadata['idx2w']) # 8002 (0~8001)\n",
        "    emb_dim = 1024\n",
        "\n",
        "    word2idx = metadata['w2idx']   # dict  word 2 index\n",
        "    idx2word = metadata['idx2w']   # list index 2 word\n",
        "\n",
        "    unk_id = word2idx['unk']   # 1\n",
        "    pad_id = word2idx['_']     # 0\n",
        "\n",
        "    start_id = src_vocab_size  # 8002\n",
        "    end_id = src_vocab_size + 1  # 8003\n",
        "\n",
        "    word2idx.update({'start_id': start_id})\n",
        "    word2idx.update({'end_id': end_id})\n",
        "    idx2word = idx2word + ['start_id', 'end_id']\n",
        "\n",
        "    src_vocab_size = tgt_vocab_size = src_vocab_size + 2\n",
        "\n",
        "    num_epochs = 1    # 迭代轮数 初始为50；\n",
        "    vocabulary_size = src_vocab_size\n",
        "    \n",
        "\n",
        "\n",
        "    def inference(seed, top_n):\n",
        "        model_.eval()\n",
        "        seed_id = [word2idx.get(w, unk_id) for w in seed.split(\" \")]\n",
        "        sentence_id = model_(inputs=[[seed_id]], seq_length=20, start_token=start_id, top_n = top_n)\n",
        "        sentence = []\n",
        "        for w_id in sentence_id[0]:\n",
        "            w = idx2word[w_id]\n",
        "            if w == 'end_id':\n",
        "                break\n",
        "            sentence = sentence + [w]\n",
        "        return sentence\n",
        "\n",
        "    decoder_seq_length = 20\n",
        "    model_ = Seq2seq(\n",
        "        decoder_seq_length = decoder_seq_length,\n",
        "        cell_enc=tf.keras.layers.GRUCell,\n",
        "        cell_dec=tf.keras.layers.GRUCell,\n",
        "        n_layer=3,\n",
        "        n_units=256,\n",
        "        embedding_layer=tl.layers.Embedding(vocabulary_size=vocabulary_size, embedding_size=emb_dim),\n",
        "        )\n",
        "    \n",
        "\n",
        "    # Uncomment below statements if you have already saved the model\n",
        "\n",
        "    # load_weights = tl.files.load_npz(name='model.npz')\n",
        "    # tl.files.assign_weights(load_weights, model_)\n",
        "\n",
        "    optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "    model_.train()\n",
        "\n",
        "    seeds = [\"happy birthday have a nice day\",\n",
        "                 \"donald trump won last nights presidential debate according to snap online polls\"]\n",
        "    for epoch in range(num_epochs):\n",
        "        model_.train()\n",
        "        trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
        "        total_loss, n_iter = 0, 0\n",
        "        for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), \n",
        "                        total=n_step, desc='Epoch[{}/{}]'.format(epoch + 1, num_epochs), leave=False):\n",
        "\n",
        "            X = tl.prepro.pad_sequences(X)\n",
        "            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n",
        "            _target_seqs = tl.prepro.pad_sequences(_target_seqs, maxlen=decoder_seq_length)\n",
        "            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n",
        "            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs, maxlen=decoder_seq_length)\n",
        "            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                ## compute outputs\n",
        "                output = model_(inputs = [X, _decode_seqs])\n",
        "                \n",
        "                output = tf.reshape(output, [-1, vocabulary_size])\n",
        "                ## compute loss and update model\n",
        "                loss = cross_entropy_seq_with_mask(logits=output, target_seqs=_target_seqs, input_mask=_target_mask)\n",
        "\n",
        "                grad = tape.gradient(loss, model_.all_weights)\n",
        "                optimizer.apply_gradients(zip(grad, model_.all_weights))\n",
        "            \n",
        "            total_loss += loss\n",
        "            n_iter += 1\n",
        "\n",
        "        # printing average loss after every epoch\n",
        "        print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, num_epochs, total_loss / n_iter))\n",
        "\n",
        "        for seed in seeds:\n",
        "            print(\"Query >\", seed)\n",
        "            top_n = 3\n",
        "            for i in range(top_n):\n",
        "                sentence = inference(seed, top_n)\n",
        "                print(\" >\", ' '.join(sentence))\n",
        "\n",
        "        tl.files.save_npz(model_.all_weights, name='model.npz')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "deployment-of-chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
